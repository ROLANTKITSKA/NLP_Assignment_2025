{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d51fda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Εγκατάσταση απαιτούμενων πακέτων\n",
    "!pip install transformers==4.37.2 sentencepiece -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "076bbedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.1-cp310-cp310-win_amd64.whl (216.1 MB)\n",
      "     ------------------------------------ 216.1/216.1 MB 741.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Collecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "     ---------------------------------------- 6.3/6.3 MB 2.4 MB/s eta 0:00:00\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "     -------------------------------------- 134.9/134.9 kB 7.8 MB/s eta 0:00:00\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 10.0 MB/s eta 0:00:00\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ------------------------------------- 536.2/536.2 kB 11.2 MB/s eta 0:00:00\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.2 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 sympy-1.14.0 torch-2.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "427a4a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7060a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great in our lives. Hope you too, to enjoy it as my deepest wishes.\n",
    "Thank your message to show our words to the doctor, as his next contract checking, to all of us.\n",
    "I got this message to see the approved message. In fact, I have received the message from the professor, to show me, this, a couple of days ago. I am very appreciated the full support of the professor, for our Springer proceedings publication.\"\"\"\n",
    "\n",
    "text2 = \"\"\"During our final discuss, I told him about the new submission — the one we were waiting since last autumn, but the updates was confusing as it not included the full feedback from reviewer or maybe editor?\n",
    "Anyway, I believe the team, although bit delay and less communication at recent days, they really tried best for paper and cooperation. We should be grateful, I mean all of us, for the acceptance and efforts until the Springer link came finally last week, I think.\n",
    "Also, kindly remind me please, if the doctor still plan for the acknowledgments section edit before he sending again. Because I didn’t see that part final yet, or maybe I missed, I apologize if so.\n",
    "Overall, let us make sure all are safe and celebrate the outcome with strong coffee and future targets\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51d8c9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--google--flan-t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# 1. Grammar/Fluency Fix (Flan-T5 small custom prompt)\n",
    "flan_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "flan_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "def flan_correct(text):\n",
    "    prompt = \"Correct the grammar and make the following text clear and natural:\\n\" + text\n",
    "    inputs = flan_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = flan_model.generate(**inputs, max_length=512)\n",
    "    return flan_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "recon1_text1 = flan_correct(text1)\n",
    "recon1_text2 = flan_correct(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "015ca232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--Vamsi--T5_Paraphrase_Paws. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nT5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 2. T5 Paraphrase (Vamsi/T5_Paraphrase_Paws)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m t5_model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVamsi/T5_Paraphrase_Paws\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m t5_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVamsi/T5_Paraphrase_Paws\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mt5_paraphrase\u001b[39m(text):\n\u001b[0;32m      6\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparaphrase: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m </s>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:832\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    830\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_fast\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2029\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2026\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2027\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[0;32m   2030\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2031\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2032\u001b[0m     init_configuration,\n\u001b[0;32m   2033\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[0;32m   2034\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2035\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2036\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2037\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   2038\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[0;32m   2039\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2040\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2261\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2259\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2261\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[0;32m   2262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   2264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2266\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:135\u001b[0m, in \u001b[0;36mT5TokenizerFast.__init__\u001b[1;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m     extra_tokens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<extra_id_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(extra_ids)]\n\u001b[0;32m    133\u001b[0m     additional_special_tokens \u001b[38;5;241m=\u001b[39m extra_tokens\n\u001b[1;32m--> 135\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    136\u001b[0m     vocab_file,\n\u001b[0;32m    137\u001b[0m     tokenizer_file\u001b[38;5;241m=\u001b[39mtokenizer_file,\n\u001b[0;32m    138\u001b[0m     eos_token\u001b[38;5;241m=\u001b[39meos_token,\n\u001b[0;32m    139\u001b[0m     unk_token\u001b[38;5;241m=\u001b[39munk_token,\n\u001b[0;32m    140\u001b[0m     pad_token\u001b[38;5;241m=\u001b[39mpad_token,\n\u001b[0;32m    141\u001b[0m     extra_ids\u001b[38;5;241m=\u001b[39mextra_ids,\n\u001b[0;32m    142\u001b[0m     additional_special_tokens\u001b[38;5;241m=\u001b[39madditional_special_tokens,\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    144\u001b[0m )\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_ids \u001b[38;5;241m=\u001b[39m extra_ids\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:114\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m TokenizerFast\u001b[38;5;241m.\u001b[39mfrom_file(fast_tokenizer_file)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslow_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# We need to create and convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1386\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[1;34m(transformer_tokenizer)\u001b[0m\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1379\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn instance of tokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be converted in a Fast tokenizer instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m No converter was found. Currently available slow->fast convertors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m     )\n\u001b[0;32m   1384\u001b[0m converter_class \u001b[38;5;241m=\u001b[39m SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n\u001b[1;32m-> 1386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverter_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconverted()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:501\u001b[0m, in \u001b[0;36mSpmConverter.__init__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m--> 501\u001b[0m     \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprotobuf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;66;03m# from .utils import sentencepiece_model_pb2 as model_pb2\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1292\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1290\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1292\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nT5Converter requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "# 2. T5 Paraphrase (Vamsi/T5_Paraphrase_Paws)\n",
    "t5_model = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")\n",
    "\n",
    "def t5_paraphrase(text):\n",
    "    input_text = \"paraphrase: \" + text + \" </s>\"\n",
    "    encoding = t5_tokenizer.encode_plus(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    output = t5_model.generate(\n",
    "        **encoding,\n",
    "        max_length=256,\n",
    "        num_beams=5,\n",
    "        num_return_sequences=1,\n",
    "        temperature=1.5\n",
    "    )\n",
    "    return t5_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "recon2_text1 = t5_paraphrase(text1)\n",
    "recon2_text2 = t5_paraphrase(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8653a92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 3. BART (facebook/bart-large-cnn) for semantic simplification\n",
    "bart = pipeline(\"text2text-generation\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "recon3_text1 = bart(text1, max_length=256, truncation=True)[0][\"generated_text\"]\n",
    "recon3_text2 = bart(text2, max_length=256, truncation=True)[0][\"generated_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08cac8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== RECONSTRUCTION RESULTS ==========\n",
      "\n",
      "=== 1. FLAN Grammar Correction ===\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'recon1_text1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m========== RECONSTRUCTION RESULTS ==========\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== 1. FLAN Grammar Correction ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText 1:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[43mrecon1_text1\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText 2:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, recon1_text2, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== 2. T5 Paraphraser ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'recon1_text1' is not defined"
     ]
    }
   ],
   "source": [
    "# Εμφάνιση Αποτελεσμάτων Μας\n",
    "print(\"\\n========== RECONSTRUCTION RESULTS ==========\\n\")\n",
    "\n",
    "print(\"=== 1. FLAN Grammar Correction ===\\n\")\n",
    "print(\"Text 1:\\n\", recon1_text1, \"\\n\")\n",
    "print(\"Text 2:\\n\", recon1_text2, \"\\n\")\n",
    "\n",
    "print(\"\\n=== 2. T5 Paraphraser ===\\n\")\n",
    "print(\"Text 1:\\n\", recon2_text1, \"\\n\")\n",
    "print(\"Text 2:\\n\", recon2_text2, \"\\n\")\n",
    "\n",
    "print(\"\\n=== 3. BART Summary/Rewrite ===\\n\")\n",
    "print(\"Text 1:\\n\", recon3_text1, \"\\n\")\n",
    "print(\"Text 2:\\n\", recon3_text2, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
